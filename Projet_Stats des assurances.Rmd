---
title: "Statistique des assurances - Projet"
author: 
- Isabelle Ajtay (41010932) 
- Smail Chabane (38012939) 
- Yuxuan Zhang (38019811)
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  pdf_document:
     toc: true
lang: fr
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

L'objectif du présent projet: on se place dans le contexte d'une entreprise d'assurance non-vie, qui dispose d'un jeu de données historiques et souhaite modéliser les sinistres (4 types distincts), les prix des polices d'assurance précédemment vendues, ainsi que la durée de vie d'un contrat d'assurance.
Nous allons utiliser la validation croisée, une des meilleures méthodes d'estimation de l'erreur des modèles.

La mtd train du package caret fait de la cv + boot, et permet d'ajuster des centaines de modèles prédictifs différents, spécifiés facilement avec l'argument method. VerboseIter donne un log du progress, pe masura ce le modèle est ajusté.

# 1. Description des données

```{r, results='hide', include=FALSE, echo=FALSE, warning = FALSE, message=FALSE}

knitr::opts_chunk$set(echo = FALSE)

#install.packages("PerformanceAnalytics")
#install.packages("vcd")

library(PerformanceAnalytics)    # pour fonction chart.Correlations
library(dplyr)
library("knitr")          # pour avoir un format table dans les sorties
library("tidyverse")
library("ggplot2")        # pour avoir de 'beaux' graphiques
library("FactoMineR")     # pour effectuer l'ACP
library("factoextra")     # pour extraire et visualiser les resultats issus de FactoMineR
library("corrplot")       # pour avoir une representation des correlations
library("ppcor")          # pour calculer les corrÃ©lations partielles
library(cowplot)          # pour mettre plusieurs graphes sur une même figure
library(questionr)        # pour des tableaux
library(scales)           # pour des plots
library(psych)
library(AER);             # Applied Econometrics with R
library(vcd)              # pour calculer le coeff. V de Cramèr


library("MASS")
library("NbClust")
library("fossil")
library("ggplot2")
library("lmtest")
library("vctrs")
```

Import des données

La procédure pour lire ligne par ligne ces données est longue. Donc nous les avons exportées dans un fichier .txt pour aller plus vite.

```{r, echo=TRUE}
cat("\f")                                          # clears the console, by sending Ctrl + L
data = read.table("data.txt", sep = " ", header=T , encoding = "UTF-8")
#data = read.table("assurance_complete_corrige.R") #, sep = "", header=T)
```

## 1.1 Analyses univariées

On a utilisé str pour afficher les informations simples concernant les variables, et summary pour afficher les données statistiques pour chaque variable.

```{r message=FALSE, warning=FALSE, include=FALSE}
attach(data)
str(data)                      # 5 352 obs, 27 vars
summary(data)

# on observe des factuers encodés comme caractères. On corrige.

```

Voici les variables:
[1] pcs        2 RUC       3 cs         4 reves      5 crevpp       6 region     7 habi
8 Ahabi        9 Atyph      10 agecat   11 Acompm     12 nbpers     13 enfants    14 Anat
15 Bauto      16 "Nbadulte"   17 Sinistre1"  18 Sinistre2 19 Sinistre3"  20 Police1
21 "Police2"    22 "Police3"    23 "durPolice1" 24 Durée"      25 NSin"       26 censure"    27 Sinistre0

On observe que les variables *pcs, cs, region, crevpp, agecat* et *habi* sont qualitatives, malgré leur apparence numérique; on les convertit en facteurs

```{r}
sort(unique(region))          #  1 2 3 4 5 7 8 9

data$pcs = as.factor ( data$pcs )
data$cs = as.factor ( data$cs )
data$Ahabi = as.factor ( data$Ahabi )
data$Atyph = as.factor( data$Atyph )
data$region = as.factor ( data$region )
data$habi = as.factor ( data$habi )
data$crevpp = as.factor ( data$crevpp )
data$agecat = as.factor ( data$agecat )
data$Acompm = as.factor(data$Acompm)
data$enfants = as.factor(data$enfants)
data$Anat = as.factor(data$Anat)
data$Anat = as.factor(data$Anat)
data$Bauto = ifelse(Bauto == "Pas de vehicule", 0, 1)
data2 = data
data2$RUC = log(data2$RUC)
```


```{r}
plot(RUC)
plot(log10(RUC))
hist(RUC)
hist(log10(RUC))
hist(Sinistre0)
hist(NSin)
hist(log(NSin))

# Verification des donnees manquantes
# install.packages("funModeling")

library ( funModeling )
# funModeling v.1.9.4 :)
# Examples and tutorials at livebook.datascienceheroes.com
df_status(data)         # pas de NA

# Statistiques descriptives
profiling_num(data)
# statdes = cbind ( profiling_num(data)[,-c(3:7,9,12:16)], max=c( max(RUC),
# max( reves ), max( Sinistre1), max( Sinistre2), max( Sinistre3),
# max( Police1), max ( Police2), max( Police3), max( durPolice1),
# max( Durée ), max( NSin )))
# statdes
# 
# max=c( max(RUC),
# max( reves ), max( Sinistre1), max( Sinistre2), max( Sinistre3),
# max( Police1), max ( Police2), max( Police3), max( durPolice1),
# max( Durée ), max( NSin ))
# max

quantile(RUC)
```

On a representé les boxplots des variables RUC et Sinistre0. Pour écraser les grandes valeurs, on utilise la fonction log.

```{r, echo=TRUE}
par(mfrow=c(2,2))
boxplot(log(data[,2]),main="Boxplot de variable RUC")
ggplot(data, aes(y=RUC, fill=Durée)) + geom_boxplot(orientation = "x") + labs(subtitle = "Revenu par unité de consommation")

hist(data[,2],main="Histogramme de variable RUC")
#  erreur ici chez Eva : stat_density requires an x or y aesthetic ggplot(data = data.frame(data[,2])) + geom_density()
boxplot(data[,27],main="Boxplot de variable Sinistre0")
hist(data[,27],main="Histogramme de variable Sinistre0")
density(data[,27])
```

```{r, echo=TRUE}
par(mfrow=c(3,2))
boxplot(data[,17],main="Boxplot de variable Sinistre1")
hist(data[,17],main="Histogramme de variable Sinistre1")
boxplot(data[,18],main="Boxplot de variable Sinistre2")
hist(data[,18],main="Histogramme de variable Sinistre2")
boxplot(data[,19],main="Boxplot de variable Sinistre3")
hist(data[,19],main="Histogramme de variable Sinistre3")
```

## 1.2 Analyses bivariées

A travers un graphique des variables numériques deux à deux, nous regardons comment évoluent les variables ensemble, et s'il y a des "tendances" reconnaissables. Par exemple, sur le graphique ci-dessous, qui contient les analyses bivariées complètes, on voit une tendance linéaire croissante (et corrélation positive significative) entre *pib* et *recc*. (fonction trouvée à ref. 7: analyse bi + corrélations).

```{r warning=FALSE, echo=FALSE, out.width="100%"}
# Cette fonction presente en plus les centre des nuages, et intervalles de confiance. Le graphique est plus colore aussi

#install.packages("psych")
library(psych)
  
# on retire des variables
pairs.panels(data2[,-c(1,2, 3,4, 6, 7, 8, 9, 11, 14, 15,16,17,18,19,20,21,22,23,24, 25,26)],         # [2:12]
         smooth = TRUE,    # If TRUE, draws loess smooths
         hist.col="#00FA9A",        # Histograms color
         show.points=TRUE, 
         stars=TRUE,       # If TRUE, adds significance level with stars; default is false
         gap=0.05,         # spacing between matrix boxes
         pch=".",          # pch symbol
         ellipses=FALSE,   # If TRUE, draws ellipses 
         scale=TRUE,      # If TRUE, scales the correlation text font
         main="Correlations les plus significatives de Sinistre0 (variables 2 par 2)", 
         method="pearson", # Correlation method (can also be "spearman" or "kendall")
         col="red")        # #ADFF2F

#     lm = FALSE,      # If TRUE, plots linear fit rather than the LOESS (smoothed) fit
#     jiggle = FALSE,  # If TRUE, data points are jittered
#     factor = 2,      # Jittering factor
#     ci = TRUE)       # If TRUE, adds confidence intervals
```

On peut constater que les variabilités des Sinistres des 3 types sont toutes grandes.

```{r, echo=TRUE}
aggregate(data[,c(17,18,19,27)],list(data[,1]),mean)
```

```{r, echo=TRUE}
#Representation des correlations
variables_quantitatives = data %>% select_if(is.numeric) %>% cor()
kable(variables_quantitatives, digits=3)
corrplot(variables_quantitatives)
```
```{r}   
# variante de Q.
datc =as.data.frame ( lapply (data2 ,as.numeric ))
correlation = cor( datc )
corrplot ( correlation )
```

```{r, echo=TRUE}
# Boxplot
# boxplot(data$column_name, main = "Boxplot of column_name", xlab = "Column name", ylab = "Values")

# Histogram
# hist(data$column_name, main = "Histogram of column_name", xlab = "Values", ylab = "Frequency", col = "blue")

# Estimateurs de la densitÃ©
# density_plot <- density(data$column_name, main = "Density Plot of column_name", xlab = "Values", ylab = "Density")
# lines(density_plot, col = "red")

# Statistiques basiques
# summary(data$column_name)

plot(variables_quantitatives)

```

```{r}
#-------------------------------------------------
#                          ACP
#-------------------------------------------------

res.pca <- PCA(variables_quantitatives, scale.unit = TRUE, ncp=5, graph = F )

res.pca

round(res.pca$eig,2)
```

```{r echo=FALSE, warning=FALSE, figures-side-biplot, fig.show="hold", out.width="50%"}
par(mar = c(4, 4, .1, .1))
fviz_pca_biplot(res.pca, axes=c(1,2), col.ind = "cos2", repel = TRUE, pointsize = 0.7, labelsize = 3, gradient.cols = c("turquoise1", "blue2", "red"))

fviz_pca_biplot(res.pca, axes=c(3,4), col.ind = "cos2", repel = TRUE, pointsize = 0.7, labelsize = 3, gradient.cols = c("turquoise1", "blue2", "red"))
```

## 1.3 Dépendances entre les variables

Si les variables explicatives sont fortement corrélées entre elles, cela peut rendre l'interprétation du coefficient de corrélation plus difficile. Nous allons vérifier la corrélation entre les variables explicatives en utilisant le coefficient V de Cramèr. On peut utiliser la fonction assocstats pour calculer les coefficients V de Cramer pour toutes les paires de variables et stocker les résultats dans une matrice, puis créer une heatmap à partir de cette matrice en utilisant la fonction heatmap :

```{r}
#' ne marche pas: Error in apply(x, 3:l, FUN = assocstats) : 
#'  'MARGIN' ne correspond pas à dim(X)
#' cramer <- as.matrix(assocstats(data))
#'heatmap(cramer, 
#'        symm = TRUE, 
#'        margins = c(dim(data),dim(data)), 
#'        main = "Heatmap des coefficients V de Cramer")
```

# 2. Modélisation des sinistres et des primes pures

## 2.1 Problème d'endogénéïté dans les variables

```{r, echo=TRUE}
# Selection des variables quantitatives 
quant_vars <- sapply(data, is.numeric)

# Matrice de correlation
cor_matrix <- cor(data[, quant_vars])
corrplot(cor_matrix)

# Tests de causalité de Granger pour toutes les paires de variables
# la fonction grangertest() permet de tester si une variable X est un prédicteur significatif d'une autre variable Y dans le cadre de modèles de régression linéaire. La fonction grangertest() prend comme entrée deux modèles de régression linéaire, l'un avec la variable de prédiction et l'autre sans. Elle renvoie une valeur-p pour le test de causalité de Granger. Si la valeur-p est inférieure à un niveau de signification donné (généralement 0,05), on peut rejeter l'hypothèse nulle selon laquelle la variable de prédiction n'améliore pas la capacité du modèle à prédire la variable de réponse.
quant_vars <- as.matrix(quant_vars)
d = data[, quant_vars]
for(i in 1:(ncol(d) - 1)){
  for(j in (i + 1):ncol(d)){

    result <- grangertest(d[,i], d[,j], order = 2)

    print(paste("Granger causality test entre ", colnames(d)[i], 
                " et ", colnames(d)[j], ":", result[2,4]))
  }
}
```

Si on fixe $\alpha = 0.05$, alors il y a une causalité entre Sinistre0 et les variables suivantes : RUC/durPolice1. La méthode des MCO donne l'estimateur le plus efficient s'il n'y a pas d'endogéneïté.

S'il y a de l'endogéneïté, OLS (MCO) va donner des résultats inconsistants. L'estimateur des variables instrumentales va être consistant, mais inéfficient.

```{r, echo=TRUE}
# Régression linéaire multiple
model1 <- lm(Sinistre0 ~ ., data = data)

# Afficher le résumé du modèle
summary(model1)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Sélection des variables
selectionAIC = step(model1, trace=TRUE)
```

```{r echo=TRUE}
summary(selectionAIC)
```

## 2.2 Modélisation de Sinistre0

## 2.3 Modélisation de Sinistre 1 ou 2 ou 3 (au moins un)

notamment pour Sinistre1 à 3 on choisira entre modèle gamma combiné à probit/logit, tobit, tobit généralisé ou double hurdle pour des variables bien choisies

## 2.4 Modèle pour le prix de Police 1 ou 2 ou 3 (au moins un)

## 2.5 Modèle retenu au final

Le choix du modèle retenu au final et les critères choisis devront être justifiés.

## IV regressions

### The four kinds of variables in IV

-   Y = outcome variables
-   X = endogenous, causal variable(s)
-   Z = instrument(s): doivent être exogenes, càd leur influence sur Y se fait seulement via leur influence sur X, la var endogene
-   W = any exogenous variables not including instruments

```{r, echo=TRUE}

```

# 3. Modélisation du nombre de sinistres et tarification des nouveaux arrivants

## 3.1 Modèle pour le nombre de sinistres, NSin

## 3.2 Méthode de tarification pour les nouveaux arrivants

# 4. Estimation des durées

## 4.1 Estimateur de Kaplan-Meier

## 4.2 Modèle de Cox
